{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import necessary packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\nimport cv2\nfrom PIL import Image\nimport shutil","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:02.146212Z","iopub.execute_input":"2025-03-09T02:29:02.146514Z","iopub.status.idle":"2025-03-09T02:29:13.585505Z","shell.execute_reply.started":"2025-03-09T02:29:02.146491Z","shell.execute_reply":"2025-03-09T02:29:13.584624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set up to use TPU environment in Kaggle\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='local')\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('use TPU')\nexcept:\n    print('use default strategy')\n    strategy = tf.distribute.get_strategy()\nprint(f'Number of replicas: {strategy.num_replicas_in_sync}')\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:13.586879Z","iopub.execute_input":"2025-03-09T02:29:13.587491Z","iopub.status.idle":"2025-03-09T02:29:13.594693Z","shell.execute_reply.started":"2025-03-09T02:29:13.587458Z","shell.execute_reply":"2025-03-09T02:29:13.593700Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Description of the problem/data","metadata":{}},{"cell_type":"markdown","source":"In this task, the objective is to learn Claude Monet's unique style, such as his color choices and brush strokes, from a series of images and transform completely different photos into Monet's style, allowing the model to generate new images in his style. This is achieved using a GAN (Generative Adversarial Network). In GANs, two models, called the Generator and the Discriminator, are trained to compete against each other, allowing both models to improve progressively. By utilizing this method, the task is to generate 7,000 to 10,000 images that can be classified as Monet's style.\n\nThe evaluation metric for this task is MiFID (Memorization-informed Fr√©chet Inception Distance). This metric penalizes the model if it directly reproduces existing images, encouraging the generation of novel and original content.\n\nThe available image formats for training are provided as jpg and tfrec. Using these images, the Generator and Discriminator models will be trained. The final generated images must be submitted in jpg format, compressed as a zip file named images.zip, and uploaded on Kaggle for evaluation.","metadata":{}},{"cell_type":"markdown","source":"The dataset is as follows:\n\nmonet_jpg: 300 Monet paintings in JPEG format, sized 256 x 256\nmonet_tfrec: 300 Monet paintings in TFRecord format, sized 256 x 256\nphoto_jpg: 7,028 photos in JPEG format, sized 256 x 256\nphoto_tfrec: 7,028 photos in TFRecord format, sized 256 x 256\nIt is also possible to prepare your own data and increase the total number of images up to 10,000.","metadata":{}},{"cell_type":"code","source":"# load data\n# IMG_PATH = KaggleDatasets().get_gcs_path()\nIMG_PATH = '/kaggle/input/gan-getting-started'\nprint(f'IMG_PATH: {IMG_PATH}')\nMONET_FILENAMES = tf.io.gfile.glob(f'{IMG_PATH}/monet_tfrec/*.tfrec')\nMONET_JPEG_FILENAMES = tf.io.gfile.glob(f'{IMG_PATH}/monet_jpg/*.jpg')\nPHOTO_FILENAMES = tf.io.gfile.glob(f'{IMG_PATH}/photo_tfrec/*.tfrec')\nPHOTO_JPEG_FILENAMES = tf.io.gfile.glob(f'{IMG_PATH}/photo_jpg/*.jpg')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:13.596584Z","iopub.execute_input":"2025-03-09T02:29:13.596849Z","iopub.status.idle":"2025-03-09T02:29:17.823620Z","shell.execute_reply.started":"2025-03-09T02:29:13.596807Z","shell.execute_reply":"2025-03-09T02:29:17.822485Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check the given files\ndef check_given_files(m_tfrec_files, m_jpeg_files, p_tfrec_files, p_jpeg_files):\n    print(f'Monet TFRecord Files: {len(m_tfrec_files)}')\n    print(f'Monet JPEG Files: {len(m_jpeg_files)}')\n    print(f'Photo TFRecord Files: {len(p_tfrec_files)}')\n    print(f'Photo JPEG Files: {len(p_jpeg_files)}')\ncheck_given_files(*[MONET_FILENAMES, MONET_JPEG_FILENAMES, PHOTO_FILENAMES, PHOTO_JPEG_FILENAMES])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:17.825223Z","iopub.execute_input":"2025-03-09T02:29:17.825548Z","iopub.status.idle":"2025-03-09T02:29:17.830628Z","shell.execute_reply.started":"2025-03-09T02:29:17.825516Z","shell.execute_reply":"2025-03-09T02:29:17.829970Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"It can be confirmed that the JPEG images are saved in 256x256 resolution with 3 RGB channels.","metadata":{"execution":{"iopub.status.busy":"2025-03-08T18:33:26.120713Z","iopub.execute_input":"2025-03-08T18:33:26.120963Z","iopub.status.idle":"2025-03-08T18:33:26.126435Z","shell.execute_reply.started":"2025-03-08T18:33:26.120938Z","shell.execute_reply":"2025-03-08T18:33:26.124900Z"}}},{"cell_type":"code","source":"def show_image_information(file_path):\n    img = Image.open(file_path)\n    width, height = img.size\n    print(f'these images have width: {width}, height: {height}')\n    print(f'these images are {img.mode}')\n    print(f'these image format is {img.format}')\nshow_image_information(MONET_JPEG_FILENAMES[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:17.831504Z","iopub.execute_input":"2025-03-09T02:29:17.831813Z","iopub.status.idle":"2025-03-09T02:29:18.353980Z","shell.execute_reply.started":"2025-03-09T02:29:17.831786Z","shell.execute_reply":"2025-03-09T02:29:18.353318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What exactly is Monet's artistic expression like?\nFirst, we will display 16 Monet paintings and 16 photographs side by side to visually observe Monet's style.\n\nIt is clear that Monet's style is significantly different from regular photographs, as the brushstrokes are distinctly visible, leaving a strong texture on the canvas.","metadata":{"execution":{"iopub.status.busy":"2025-03-08T20:30:25.344888Z","iopub.execute_input":"2025-03-08T20:30:25.345144Z","iopub.status.idle":"2025-03-08T20:30:25.351072Z","shell.execute_reply.started":"2025-03-08T20:30:25.345115Z","shell.execute_reply":"2025-03-08T20:30:25.348850Z"}}},{"cell_type":"code","source":"# define functions which is related to load dataset and read, decode TFRecord \ndef decode_image(image, image_size=(256, 256)):\n    max_intensity = 255\n    image = tf.image.decode_jpeg(image, channels=3) # 0 to 255\n    image = (tf.cast(image, tf.float32) / (max_intensity / 2)) # 0 to 2\n    image = image - 1 # -1 to 1\n    image = tf.reshape(image, [*image_size, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\ndef load_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:18.354798Z","iopub.execute_input":"2025-03-09T02:29:18.355057Z","iopub.status.idle":"2025-03-09T02:29:18.360291Z","shell.execute_reply.started":"2025-03-09T02:29:18.355027Z","shell.execute_reply":"2025-03-09T02:29:18.359529Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define datasets of monet and photo\nmonet_ds = load_dataset(MONET_FILENAMES).batch(1)\nphoto_ds = load_dataset(PHOTO_FILENAMES).batch(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:18.361086Z","iopub.execute_input":"2025-03-09T02:29:18.361305Z","iopub.status.idle":"2025-03-09T02:29:19.313243Z","shell.execute_reply.started":"2025-03-09T02:29:18.361287Z","shell.execute_reply":"2025-03-09T02:29:19.312526Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define the showing dataset function\ndef show_dataset(dataset, label=None):\n    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n    data_iterator = iter(dataset)\n    for ax in axes.flatten():\n        img = next(data_iterator)\n        ax.imshow(img[0] * 0.5 + 0.5) # scaling to [0, 1] from [-1, 1]\n        ax.axis(\"off\")\n    plt.tight_layout(rect=[0, 0, 1, 0.97])\n    fig.suptitle(label)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:19.315513Z","iopub.execute_input":"2025-03-09T02:29:19.315751Z","iopub.status.idle":"2025-03-09T02:29:19.320213Z","shell.execute_reply.started":"2025-03-09T02:29:19.315731Z","shell.execute_reply":"2025-03-09T02:29:19.319416Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show monet images\nshow_dataset(monet_ds, label='Monet')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:19.321601Z","iopub.execute_input":"2025-03-09T02:29:19.321793Z","iopub.status.idle":"2025-03-09T02:29:21.695667Z","shell.execute_reply.started":"2025-03-09T02:29:19.321776Z","shell.execute_reply":"2025-03-09T02:29:21.694347Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show the photo images\nshow_dataset(photo_ds, label='Photo')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:21.696749Z","iopub.execute_input":"2025-03-09T02:29:21.696994Z","iopub.status.idle":"2025-03-09T02:29:23.614063Z","shell.execute_reply.started":"2025-03-09T02:29:21.696975Z","shell.execute_reply":"2025-03-09T02:29:23.613159Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this task, we will use CycleGAN to transform these real photos into Monet-style images.","metadata":{}},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"markdown","source":"We will use a U-Net architecture for the CycleGAN.\n\n## U-Net Architecture\nU-Net performs down-sampling to capture a larger receptive field using convolution layers and then uses up-sampling to restore the image resolution. However, during the up-sampling process, some spatial information may be lost. To mitigate this, skip connections are added between corresponding layers in the down-sampling and up-sampling stages to directly pass resolution information, helping the model maintain fine-grained details.\n\n## Initializer Settings\nIn the [tutorial](https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial) that we referred to, the random_normal_initializer was used as the initializer for both the Generator and Discriminator.\nHowever, in this experiment, we will use:\n- Orthogonal Initializer for the Generator\n- VarianceScaling Initializer for the Discriminator\n\n### Orthogonal Initializer (for Generator)\nThe Orthogonal Initializer is designed to stabilize training by ensuring the eigenvalues of the weight matrix remain close to 1. This prevents gradient vanishing or exploding issues during backpropagation, even after multiple matrix multiplications. By maintaining stable gradients, we can expect a more stable and efficient training process.\n\nReference: [Orthogonal Initialization](https://smerity.com/articles/2016/orthogonal_init.html)\n\n### VarianceScaling Initializer (for Discriminator)\nThe VarianceScaling Initializer scales the randomly sampled weight values based on the input size of the layer. This method helps to keep the variance of activations and gradients stable, reducing the risk of gradient vanishing or exploding during training.\n\nBy applying these initializers, we aim to enhance the stability and performance of the CycleGAN model.","metadata":{}},{"cell_type":"markdown","source":"## Down Sampling Layer\nWe will create the down-sampling layer for the U-Net architecture.\nThere are slight differences depending on whether Instance Normalization is applied, but the overall structure is as follows:\n\n- Conv2D(stride=2) ‚Üí (Instance Normalization) ‚Üí LeakyReLU\nWe use the HeNormal initializer for the Conv2D layer to avoid vanishing gradients during training.\n\n\n### Weight Initialization in HeNormal\nIn the HeNormal initializer, the weights are initialized as follows:\n$$\nW \\sim \\mathcal{N} \\left( 0, \\sqrt{\\frac{2}{n_{\\text{in}}}} \\right)\n$$\n$$\nn_{\\text{in}}Ôºö\\text{The number of input units for that layer}\n$$\n$$\n\\text{Standard deviation: } \\sigma = \\sqrt{\\frac{2}{n_{\\text{in}}}}\n$$\n\nBy using HeNormal initialization, the model can achieve faster and more stable convergence by mitigating gradient vanishing issues.","metadata":{}},{"cell_type":"code","source":"# define downsampling layer function\ndef downsample(filters, size, apply_instancenorm=True):\n    # initializer = tf.random_normal_initializer(0., 0.02)\n    initializer = tf.keras.initializers.HeNormal()\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    model = keras.Sequential()\n    model.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, \n                             use_bias=False))\n\n    if apply_instancenorm:\n        model.add(layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init)) # Instance Normalization\n\n    model.add(layers.LeakyReLU())\n    return model","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T04:16:39.068Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Up Sampling Layer\nWe will create the up-sampling layer for the U-Net architecture.\nThere are some differences depending on whether Dropout is applied, but the overall structure is as follows:\n\n- Conv2DTranspose ‚Üí Instance Normalization ‚Üí (Dropout(0.5)) ‚Üí ReLU\nWe use the HeNormal initializer for the Conv2DTranspose layer to prevent vanishing gradients during training. This initialization helps stabilize and accelerate the learning process.","metadata":{}},{"cell_type":"code","source":"# define upsampling layer function\ndef upsample(filters, size, apply_dropout=False):\n    # initializer = tf.random_normal_initializer(0., 0.02)\n    initializer = tf.keras.initializers.HeNormal()\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    model = keras.Sequential()\n    model.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    model.add(layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init)) # Instance Normalization\n\n    if apply_dropout:\n        model.add(layers.Dropout(0.5))\n\n    model.add(layers.LeakyReLU())\n    return model","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-03-09T02:29:23.623496Z","iopub.execute_input":"2025-03-09T02:29:23.623786Z","iopub.status.idle":"2025-03-09T02:29:23.663913Z","shell.execute_reply.started":"2025-03-09T02:29:23.623761Z","shell.execute_reply":"2025-03-09T02:29:23.663003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generator\nThe Generator is constructed using a U-Net structure, which combines the Down Sampling Layer and Up Sampling Layer.\nThe overall architecture is as follows:\n\n- Input(256, 256, 3) -> downsample(64) instance norm„Å™„Åó -> downsample(128) -> downsample(256) x 4\n-> downsample(512): center\n-> upsample(256) x4 -> upsample(128) -> upsample(64) \n-> Conv2DTranspose(stride=2)","metadata":{}},{"cell_type":"code","source":"# define Generator\ndef Generator():\n    channels = 3\n    inputs = layers.Input(shape=[256, 256, channels])\n\n    # bs = batch size\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n    ]\n\n    center_layer = downsample(512, 4) # (bs, 1, 1, 512)\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4),  # (bs, 128, 128, 128)\n    ]\n\n    # initializer = tf.random_normal_initializer(0., 0.02)\n    initializer = tf.keras.initializers.Orthogonal(gain=0.02)\n    last = layers.Conv2DTranspose(filters=channels, \n                                  kernel_size=4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n\n    # Build Generator\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    # center layer\n    x = center_layer(x)\n    skips = reversed(skips)\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    # last layer\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:23.664812Z","iopub.execute_input":"2025-03-09T02:29:23.665139Z","iopub.status.idle":"2025-03-09T02:29:23.673612Z","shell.execute_reply.started":"2025-03-09T02:29:23.665118Z","shell.execute_reply":"2025-03-09T02:29:23.672679Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Discriminator\nDiscriminator can distinguish between images generated by the Generator and real images. The structure is slightly modified and defined as follows:\n\n- Input(256, 256, 3) -> downsample(64) instance norm„Å™„Åó -> downsample(128) -> downsample(256) -> ZeroPadding2D -> Conv2D(512) -> Instance Normalization -> Dropout(0.2) ->LeakyReLU-> ZeroPadding2D -> Conv2D","metadata":{}},{"cell_type":"code","source":"# define Discriminator\ndef Discriminator():\n    # initializer = tf.random_normal_initializer(0., 0.02)\n    initializer = tf.keras.initializers.VarianceScaling(scale=2.0)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    input = layers.Input(shape=[256, 256, 3], name='input_image')\n\n    # input layer\n    x = input\n\n    down1 = downsample(64, 4, apply_instancenorm=False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = layers.GroupNormalization(groups=-1, gamma_initializer=gamma_init)(conv) # Instance Normalization\n    dropout = layers.Dropout(0.2)(norm1)\n\n    leaky_relu = layers.LeakyReLU()(dropout)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=input, outputs=last)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:23.674539Z","iopub.execute_input":"2025-03-09T02:29:23.674828Z","iopub.status.idle":"2025-03-09T02:29:23.691904Z","shell.execute_reply.started":"2025-03-09T02:29:23.674806Z","shell.execute_reply":"2025-03-09T02:29:23.691064Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setup monet, photo generator and monet, photo discriminator\nwith strategy.scope():\n    monet_generator = Generator() # transforms photos to Monet-esque paintings\n    photo_generator = Generator() # transforms Monet paintings to be more like photos\n\n    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n    photo_discriminator = Discriminator() # differentiates real photos and generated photos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:23.692815Z","iopub.execute_input":"2025-03-09T02:29:23.693141Z","iopub.status.idle":"2025-03-09T02:29:25.661689Z","shell.execute_reply.started":"2025-03-09T02:29:23.693121Z","shell.execute_reply":"2025-03-09T02:29:25.660795Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Confirm that the Generator is initialized.\nSince the Generator has not been trained yet, it outputs images in grayscale.","metadata":{}},{"cell_type":"code","source":"def show_initial_generator(dataset):\n    photo = next(iter(dataset))\n    to_monet = monet_generator(photo)\n    plt.figure(figsize=(16, 16))\n    plt.subplot(1, 2, 1)\n    plt.title(\"Original Photo\")\n    plt.imshow(photo[0] * 0.5 + 0.5)\n    plt.gca().get_xaxis().set_visible(False)\n    plt.gca().get_yaxis().set_visible(False)\n    \n    plt.subplot(1, 2, 2)\n    plt.title(\"Monet-esque Photo\")\n    plt.imshow(to_monet[0] * 0.5 + 0.5)\n    plt.gca().get_xaxis().set_visible(False)\n    plt.gca().get_yaxis().set_visible(False)\n    plt.show()\nshow_initial_generator(photo_ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:25.662553Z","iopub.execute_input":"2025-03-09T02:29:25.662855Z","iopub.status.idle":"2025-03-09T02:29:28.211448Z","shell.execute_reply.started":"2025-03-09T02:29:25.662833Z","shell.execute_reply":"2025-03-09T02:29:28.210630Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CycleGAN model\nI will construct the CycleGAN.\n\nIn CycleGAN, both the Generator and Discriminator are used to transform the original photo into a Monet-styled image, and then perform the reverse transformation. The difference between the transformed image and the original image is defined as the cycle-consistency loss. These losses are used to train the model.","metadata":{}},{"cell_type":"code","source":"# define CycleGAN class\nclass CycleGan():\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10,\n    ):\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n\n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n\n    @tf.function\n    def distributed_train_step(self, batch_data):\n        losses = strategy.run(self.train_step, args=(batch_data,))\n        return losses\n\n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n            \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }\n\n    def plot_generated_image(self, real_potho):\n        to_monet = self.m_gen(real_potho)\n        plt.figure(figsize=(16, 16))\n        plt.subplot(1, 2, 1)\n        plt.title(\"Original Photo\")\n        plt.imshow(real_potho[0] * 0.5 + 0.5)\n        plt.gca().get_xaxis().set_visible(False)\n        plt.gca().get_yaxis().set_visible(False)\n        \n        plt.subplot(1, 2, 2)\n        plt.title(\"Monet-esque Photo\")\n        plt.imshow(to_monet[0] * 0.5 + 0.5)\n        plt.gca().get_xaxis().set_visible(False)\n        plt.gca().get_yaxis().set_visible(False)\n        plt.show()\n\n    def fit(self, batch_data, epochs):\n        for epoch in range(epochs):\n            dataset_length = sum(1 for _ in batch_data)\n            for data in tqdm(batch_data, total=dataset_length):\n                loss = self.distributed_train_step(data)\n            # show the images\n            if epoch % 10 == 0:\n                _ , real_photo = list(batch_data.take(1))[0]\n                self.plot_generated_image(real_photo)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.212497Z","iopub.execute_input":"2025-03-09T02:29:28.212828Z","iopub.status.idle":"2025-03-09T02:29:28.234662Z","shell.execute_reply.started":"2025-03-09T02:29:28.212793Z","shell.execute_reply":"2025-03-09T02:29:28.233800Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss functions\n- Discriminator Loss: Measures whether the discriminator correctly identifies a fake image.\n- Generator Loss: Measures whether the generator successfully makes the fake image appear as real to the discriminator.\n- Cycle Loss: Measures how much the original image can be transformed and then returned to its original form after the reverse transformation.\n- Identity Loss: Measures how similar the original image is to the transformed image when passed through the same generator without style transfer.","metadata":{}},{"cell_type":"code","source":"# define discriminator loss function\nwith strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss_binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n        real_loss = real_loss_binary_cross_entropy(tf.ones_like(real), real)\n\n        generated_loss_binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n        generated_loss = generated_loss_binary_cross_entropy(tf.zeros_like(generated), generated)\n\n        total_disc_loss = real_loss + generated_loss\n        return total_disc_loss * 0.5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.235695Z","iopub.execute_input":"2025-03-09T02:29:28.235998Z","iopub.status.idle":"2025-03-09T02:29:28.262715Z","shell.execute_reply.started":"2025-03-09T02:29:28.235967Z","shell.execute_reply":"2025-03-09T02:29:28.261987Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define generator loss function\nwith strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.263593Z","iopub.execute_input":"2025-03-09T02:29:28.263907Z","iopub.status.idle":"2025-03-09T02:29:28.281025Z","shell.execute_reply.started":"2025-03-09T02:29:28.263875Z","shell.execute_reply":"2025-03-09T02:29:28.280386Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define cycle loss function\nwith strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n        return LAMBDA * loss1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.281814Z","iopub.execute_input":"2025-03-09T02:29:28.282122Z","iopub.status.idle":"2025-03-09T02:29:28.296469Z","shell.execute_reply.started":"2025-03-09T02:29:28.282093Z","shell.execute_reply":"2025-03-09T02:29:28.295744Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define identity loss\nwith strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.297190Z","iopub.execute_input":"2025-03-09T02:29:28.297478Z","iopub.status.idle":"2025-03-09T02:29:28.312442Z","shell.execute_reply.started":"2025-03-09T02:29:28.297452Z","shell.execute_reply":"2025-03-09T02:29:28.311780Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# instantiate optimizers\nwith strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.313192Z","iopub.execute_input":"2025-03-09T02:29:28.313471Z","iopub.status.idle":"2025-03-09T02:29:28.336926Z","shell.execute_reply.started":"2025-03-09T02:29:28.313444Z","shell.execute_reply":"2025-03-09T02:29:28.336374Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.339819Z","iopub.execute_input":"2025-03-09T02:29:28.340034Z","iopub.status.idle":"2025-03-09T02:29:28.343930Z","shell.execute_reply.started":"2025-03-09T02:29:28.339993Z","shell.execute_reply":"2025-03-09T02:29:28.342932Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run the CycleGAN.\nThis time, we will set the number of epochs to 300.","metadata":{}},{"cell_type":"code","source":"cycle_gan_model.fit(\n        tf.data.Dataset.zip((monet_ds, photo_ds)),\n        epochs=300\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T02:29:28.345062Z","iopub.execute_input":"2025-03-09T02:29:28.345290Z","execution_failed":"2025-03-09T04:16:39.067Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results and Analysis\nAfter training, verify if the generated images have adopted Monet's style.","metadata":{}},{"cell_type":"code","source":"# show generated images\ndef show_generated_images(dataset, label=None):\n    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n    data_iterator = iter(dataset)\n    for ax in axes.flatten():\n        img = next(data_iterator)\n        prediction = monet_generator(img, training=False)[0].numpy()\n        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n        ax.imshow(prediction)\n        ax.axis(\"off\")\n    plt.tight_layout(rect=[0, 0, 1, 0.97])\n    fig.suptitle(label)\n    plt.show()\nshow_generated_images(photo_ds, 'Generated Monet styled images')","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"execution_failed":"2025-03-09T04:16:39.068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show original images\nshow_dataset(photo_ds, 'Original images')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T04:16:39.068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion ","metadata":{}},{"cell_type":"markdown","source":"This time, images were generated using a CycleGAN model.\nBy utilizing the U-Net architecture, the image information is connected through skip connections, which allows the model to produce Monet-like images even with deep layers.\n\nSince we didn't incorporate the Self-Attention mechanism this time, it could be an interesting challenge in the future to explore which parts of the image to focus on in order to generate more Monet-style images.","metadata":{}},{"cell_type":"markdown","source":"# Create submission file","metadata":{}},{"cell_type":"code","source":"import PIL\n! mkdir ../images","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T04:16:39.068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 1\nfor img in photo_ds:\n    prediction = monet_generator(img, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(prediction)\n    im.save(\"../images/\" + str(i) + \".jpg\")\n    i += 1","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T04:16:39.068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-09T04:16:39.068Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial\n- https://www.kaggle.com/code/wendykan/demo-mifid-metric-for-dog-image-generation-comp\n- https://www.claude-monet.com\n- https://jonathan-hui.medium.com/gan-cyclegan-6a50e7600d7","metadata":{}}]}